import warnings
from typing import Optional, Tuple

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch import Tensor
from torch.nn.init import constant_, xavier_normal_, xavier_uniform_

from .MAAM import MultiscaleAttentionAggregationModule


class MultiheadAttention(nn.Module):
    bias_k: Optional[torch.Tensor]
    bias_v: Optional[torch.Tensor]

    def __init__(
        self,
        embed_dim,
        num_heads,
        dropout=0.0,
        bias=True,
        add_bias_kv=False,
        add_zero_attn=False,
        kdim=None,
        vdim=None,
    ):
        """多头注意力机制

        Args:
            embed_dim (int): 嵌入投影维度
            num_heads (int): 头数
            dropout (float, optional): dropout参数. Defaults to 0.0.
            bias (bool, optional): 是否使用bias. Defaults to True.
            add_bias_kv (bool, optional): 是否向kv添加bias. Defaults to False.
            add_zero_attn (bool, optional): 使用0填充attn的标志位. Defaults to False.
            kdim (int, optional): k的维度. Defaults to None.
            vdim (int, optional): v的维度. Defaults to None.
        """
        super(MultiheadAttention, self).__init__()
        self.embed_dim = embed_dim
        self.kdim = kdim if kdim is not None else embed_dim
        self.vdim = vdim if vdim is not None else embed_dim

        # qkv是否同维度
        self._qkv_same_embed_dim = self.kdim == embed_dim and self.vdim == embed_dim

        self.num_heads = num_heads
        self.dropout = dropout
        # 将嵌入矩阵按头数切分
        self.head_dim = embed_dim // num_heads
        assert (self.head_dim * num_heads == self.embed_dim
                ), "embed_dim must be divisible by num_heads"

        # 不同维度则分别创建权重矩阵
        if self._qkv_same_embed_dim is False:
            self.q_proj_weight = nn.Parameter(
                torch.Tensor(embed_dim, embed_dim))
            self.k_proj_weight = nn.Parameter(
                torch.Tensor(embed_dim, self.kdim))
            self.v_proj_weight = nn.Parameter(
                torch.Tensor(embed_dim, self.vdim))
            self.register_parameter("in_proj_weight", None)
        else:
            self.in_proj_weight = nn.Parameter(
                torch.empty(3 * embed_dim, embed_dim))
            self.register_parameter("q_proj_weight", None)
            self.register_parameter("k_proj_weight", None)
            self.register_parameter("v_proj_weight", None)

        if bias:
            self.in_proj_bias = nn.Parameter(torch.empty(3 * embed_dim))
        else:
            self.register_parameter("in_proj_bias", None)

        # 输出的线性层
        self.out_proj = nn.Linear(embed_dim, embed_dim)

        if add_bias_kv:
            self.bias_k = nn.Parameter(torch.empty(1, 1, embed_dim))
            self.bias_v = nn.Parameter(torch.empty(1, 1, embed_dim))
        else:
            self.bias_k = self.bias_v = None

        self.add_zero_attn = add_zero_attn

        self._reset_parameters()

    def _reset_parameters(self):
        """初始化参数
        """
        # 使用xavier初始化参数，保证数值稳定性
        if self._qkv_same_embed_dim:
            xavier_uniform_(self.in_proj_weight)
        else:
            xavier_uniform_(self.q_proj_weight)
            xavier_uniform_(self.k_proj_weight)
            xavier_uniform_(self.v_proj_weight)

        # 初始化偏置
        if self.in_proj_bias is not None:
            constant_(self.in_proj_bias, 0.0)
            constant_(self.out_proj.bias, 0.0)
        if self.bias_k is not None:
            xavier_normal_(self.bias_k)
        if self.bias_v is not None:
            xavier_normal_(self.bias_v)

    # 模型参数序列化
    def __setstate__(self, state):
        # Support loading old MultiheadAttention checkpoints generated by v1.1.0
        if "_qkv_same_embed_dim" not in state:
            state["_qkv_same_embed_dim"] = True

        super(MultiheadAttention, self).__setstate__(state)

    def forward(
        self,
        query: Tensor,
        key: Tensor,
        value: Tensor,
        maam: Optional[MultiscaleAttentionAggregationModule] = None,
        key_padding_mask: Optional[Tensor] = None,
        need_weights: bool = True,
        attn_mask: Optional[Tensor] = None,
    ) -> Tuple[Tensor, Optional[Tensor]]:
        # 分为两类
        if not self._qkv_same_embed_dim:
            return multi_head_attention_forward(
                query,
                key,
                value,
                maam,
                self.embed_dim,
                self.num_heads,
                self.in_proj_weight,
                self.in_proj_bias,
                self.bias_k,
                self.bias_v,
                self.add_zero_attn,
                self.dropout,
                self.out_proj.weight,
                self.out_proj.bias,
                training=self.training,
                key_padding_mask=key_padding_mask,
                need_weights=need_weights,
                attn_mask=attn_mask,
                use_separate_proj_weight=True,
                q_proj_weight=self.q_proj_weight,
                k_proj_weight=self.k_proj_weight,
                v_proj_weight=self.v_proj_weight,
            )
        else:
            return multi_head_attention_forward(
                query,
                key,
                value,
                maam,
                self.embed_dim,
                self.num_heads,
                self.in_proj_weight,
                self.in_proj_bias,
                self.bias_k,
                self.bias_v,
                self.add_zero_attn,
                self.dropout,
                self.out_proj.weight,
                self.out_proj.bias,
                training=self.training,
                key_padding_mask=key_padding_mask,
                need_weights=need_weights,
                attn_mask=attn_mask,
            )


def multi_head_attention_forward(
    query: Tensor,
    key: Tensor,
    value: Tensor,
    maam: Optional[MultiscaleAttentionAggregationModule],
    embed_dim_to_check: int,
    num_heads: int,
    in_proj_weight: Tensor,
    in_proj_bias: Tensor,
    bias_k: Optional[Tensor],
    bias_v: Optional[Tensor],
    add_zero_attn: bool,
    dropout_p: float,
    out_proj_weight: Tensor,
    out_proj_bias: Tensor,
    training: bool = True,
    key_padding_mask: Optional[Tensor] = None,
    need_weights: bool = True,
    attn_mask: Optional[Tensor] = None,
    use_separate_proj_weight: bool = False,
    q_proj_weight: Optional[Tensor] = None,
    k_proj_weight: Optional[Tensor] = None,
    v_proj_weight: Optional[Tensor] = None,
    static_k: Optional[Tensor] = None,
    static_v: Optional[Tensor] = None,
) -> Tuple[Tensor, Optional[Tensor]]:
    """多头注意力机制处理函数

    Args:
        query (Tensor): 查询(q, n, l)
        key (Tensor): 键
        value (Tensor): 值
        maam (Optional[AttentionRefinementModule]): 是否使用MAAM模块
        embed_dim_to_check (int): 嵌入向量维度
        num_heads (int): 注意力头数
        in_proj_weight (Tensor): 整合权重
        in_proj_bias (Tensor): 偏置
        bias_k (Optional[Tensor]): k的偏置
        bias_v (Optional[Tensor]): v的偏置
        add_zero_attn (bool): 标志位
        dropout_p (float): dropout参数
        out_proj_weight (Tensor): 输出层权重
        out_proj_bias (Tensor): 输出层偏置
        training (bool, optional): 是否为训练时. Defaults to True.
        key_padding_mask (Optional[Tensor], optional): key的蒙版. Defaults to None.
        need_weights (bool, optional): 是否需要输出注意力权重. Defaults to True.
        attn_mask (Optional[Tensor], optional): 注意力蒙版. Defaults to None.
        use_separate_proj_weight (bool, optional): 是否使用分离权重. Defaults to False.
        q_proj_weight (Optional[Tensor], optional): q的分离权重. Defaults to None.
        k_proj_weight (Optional[Tensor], optional): k的分离权重. Defaults to None.
        v_proj_weight (Optional[Tensor], optional): v的分离权重. Defaults to None.
        static_k (Optional[Tensor], optional): k的固定值. Defaults to None.
        static_v (Optional[Tensor], optional): v的固定值. Defaults to None.

    Raises:
        RuntimeError: _description_
        RuntimeError: _description_
        RuntimeError: _description_

    Returns:
        Tuple[Tensor, Optional[Tensor]]: 输出注意力结果以及注意力权重
    """
    # print("Attention query: ", query.shape)
    # print("Attention key: ", key.shape)
    # print("Attention value: ", value.shape)
    tgt_len, bsz, embed_dim = query.size()
    assert embed_dim == embed_dim_to_check
    # allow MHA to have different sizes for the feature dimension
    assert key.size(0) == value.size(0) and key.size(1) == value.size(1)

    head_dim = embed_dim // num_heads
    assert head_dim * num_heads == embed_dim, "embed_dim must be divisible by num_heads"

    # 缩放点积注意力机制使用的缩放大小
    scaling = float(head_dim)**-0.5

    if not use_separate_proj_weight:
        # q,k,v全部相等时
        if (query is key or torch.equal(
                query, key)) and (key is value or torch.equal(key, value)):
            # self-attention
            q, k, v = F.linear(query, in_proj_weight,
                               in_proj_bias).chunk(3, dim=-1)  # [l, 2b ,d]

        # 只有kay和value相等，说明是由Encoder向Decoder传入key和value，q是Decoder中输入的
        elif key is value or torch.equal(key, value):
            # encoder-decoder attention
            # This is inline in_proj function with in_proj_weight and in_proj_bias
            _b = in_proj_bias
            _start = 0
            _end = embed_dim
            _w = in_proj_weight[_start:_end, :]
            if _b is not None:
                _b = _b[_start:_end]
            q = F.linear(query, _w, _b)  # [l, 2b ,d]

            if key is None:
                assert value is None
                k = None
                v = None
            else:

                # This is inline in_proj function with in_proj_weight and in_proj_bias
                _b = in_proj_bias
                _start = embed_dim
                _end = None
                _w = in_proj_weight[_start:, :]
                if _b is not None:
                    _b = _b[_start:]
                # 按最后一个维度拆分为k，v
                out_k_v = F.linear(key, _w, _b)
                k, v = out_k_v.chunk(2, dim=-1)  # [l, 2b ,d]

        # 否则q,k,v均不相同
        else:
            # This is inline in_proj function with in_proj_weight and in_proj_bias
            _b = in_proj_bias
            _start = 0
            _end = embed_dim
            _w = in_proj_weight[_start:_end, :]
            if _b is not None:
                _b = _b[_start:_end]
            q = F.linear(query, _w, _b)

            # This is inline in_proj function with in_proj_weight and in_proj_bias
            _b = in_proj_bias
            _start = embed_dim
            _end = embed_dim * 2
            _w = in_proj_weight[_start:_end, :]
            if _b is not None:
                _b = _b[_start:_end]
            k = F.linear(key, _w, _b)

            # This is inline in_proj function with in_proj_weight and in_proj_bias
            _b = in_proj_bias
            _start = embed_dim * 2
            _end = None
            _w = in_proj_weight[_start:, :]
            if _b is not None:
                _b = _b[_start:]
            v = F.linear(value, _w, _b)
    else:
        # _unwrap_optional为检测是否为None的函数
        q_proj_weight_non_opt = torch.jit._unwrap_optional(q_proj_weight)
        len1, len2 = q_proj_weight_non_opt.size()
        assert len1 == embed_dim and len2 == query.size(-1)

        k_proj_weight_non_opt = torch.jit._unwrap_optional(k_proj_weight)
        len1, len2 = k_proj_weight_non_opt.size()
        assert len1 == embed_dim and len2 == key.size(-1)

        v_proj_weight_non_opt = torch.jit._unwrap_optional(v_proj_weight)
        len1, len2 = v_proj_weight_non_opt.size()
        assert len1 == embed_dim and len2 == value.size(-1)

        if in_proj_bias is not None:
            q = F.linear(query, q_proj_weight_non_opt,
                         in_proj_bias[0:embed_dim])
            k = F.linear(key, k_proj_weight_non_opt,
                         in_proj_bias[embed_dim:(embed_dim * 2)])
            v = F.linear(value, v_proj_weight_non_opt,
                         in_proj_bias[(embed_dim * 2):])
        else:
            q = F.linear(query, q_proj_weight_non_opt, in_proj_bias)
            k = F.linear(key, k_proj_weight_non_opt, in_proj_bias)
            v = F.linear(value, v_proj_weight_non_opt, in_proj_bias)

    # 对q进行缩放
    q = q * scaling  # [l, 2b, d]

    if attn_mask is not None:
        assert (
            attn_mask.dtype == torch.float32
            or attn_mask.dtype == torch.float64
            or attn_mask.dtype == torch.float16
            or attn_mask.dtype == torch.uint8 or attn_mask.dtype == torch.bool
        ), "Only float, byte, and bool types are supported for attn_mask, not {}".format(
            attn_mask.dtype)
        if attn_mask.dtype == torch.uint8:
            warnings.warn(
                "Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
            )
            attn_mask = attn_mask.to(torch.bool)

        if attn_mask.dim() == 2:
            attn_mask = attn_mask.unsqueeze(0)
            if list(attn_mask.size()) != [1, query.size(0), key.size(0)]:
                raise RuntimeError(
                    "The size of the 2D attn_mask is not correct.")
        elif attn_mask.dim() == 3:
            if list(attn_mask.size()) != [
                    bsz * num_heads,
                    query.size(0), key.size(0)
            ]:
                raise RuntimeError(
                    "The size of the 3D attn_mask is not correct.")
        else:
            raise RuntimeError(
                "attn_mask's dimension {} is not supported".format(
                    attn_mask.dim()))
        # attn_mask's dim is 3 now.

    # convert ByteTensor key_padding_mask to bool
    if key_padding_mask is not None and key_padding_mask.dtype == torch.uint8:
        warnings.warn(
            "Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead."
        )
        key_padding_mask = key_padding_mask.to(torch.bool)

    if bias_k is not None and bias_v is not None:
        if static_k is None and static_v is None:
            k = torch.cat([k, bias_k.repeat(1, bsz, 1)])
            v = torch.cat([v, bias_v.repeat(1, bsz, 1)])
            if attn_mask is not None:
                attn_mask = F.pad(attn_mask, (0, 1))
            if key_padding_mask is not None:
                key_padding_mask = F.pad(key_padding_mask, (0, 1))
        else:
            assert static_k is None, "bias cannot be added to static key."
            assert static_v is None, "bias cannot be added to static value."
    else:
        assert bias_k is None
        assert bias_v is None

    # print("Attention bsz: ", bsz)
    # print("Attention num_head: ", num_heads)
    # contiguous返回与原tensor数据相同的连续内存tensor
    q = q.contiguous().view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)  # [2b * num_heads, l, head_dim(d / num_heads)]
    if k is not None:
        k = k.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)  # [2b * num_heads, src_l, head_dim(d / num_heads)]
    if v is not None:
        v = v.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)  # [2b * num_heads, src_l, head_dim(d / num_heads)]
    # print("attention q shape: ", q.shape)
    # print("attention k shape: ", k.shape)
    # print("attention v shape: ", v.shape)
    if static_k is not None:
        assert static_k.size(0) == bsz * num_heads
        assert static_k.size(2) == head_dim
        k = static_k

    if static_v is not None:
        assert static_v.size(0) == bsz * num_heads
        assert static_v.size(2) == head_dim
        v = static_v

    src_len = k.size(1)

    # if key_padding_mask is not None:
    #     print("attention key: ", k.shape)
    #     print("attention key_mask: ", key_padding_mask.shape)

    if key_padding_mask is not None:
        assert key_padding_mask.size(0) == bsz
        assert key_padding_mask.size(1) == src_len

    if add_zero_attn:
        src_len += 1
        k = torch.cat(
            [
                k,
                torch.zeros((k.size(0), 1) + k.size()[2:],
                            dtype=k.dtype,
                            device=k.device),
            ],
            dim=1,
        )
        v = torch.cat(
            [
                v,
                torch.zeros((v.size(0), 1) + v.size()[2:],
                            dtype=v.dtype,
                            device=v.device),
            ],
            dim=1,
        )
        if attn_mask is not None:
            attn_mask = F.pad(attn_mask, (0, 1))
        if key_padding_mask is not None:
            key_padding_mask = F.pad(key_padding_mask, (0, 1))
    # 计算两个矩阵的矩阵乘法，即q * k
    attn_output_weights = torch.bmm(q, k.transpose(1, 2))  # [2b * num_head, l, src_l]
    assert list(
        attn_output_weights.size()) == [bsz * num_heads, tgt_len, src_len]

    # 掩码softmax
    def mask_softmax_dropout(dots):
        if attn_mask is not None:  # [src_l, src_l] triu_(1)
            if attn_mask.dtype == torch.bool:
                dots.masked_fill_(attn_mask, float("-inf"))
            else:
                dots += attn_mask

        if key_padding_mask is not None:
            dots = dots.view(bsz, num_heads, tgt_len, src_len)  # [2b, num_head, l, src_l]
            dots = dots.masked_fill(
                key_padding_mask.unsqueeze(1).unsqueeze(2),  # [2b, 1, 1, src_l]
                float("-inf"),
            )
            dots = dots.view(bsz * num_heads, tgt_len, src_len)  # [2b * num_head, l, src_l]

        attn = F.softmax(dots, dim=-1)
        attn = F.dropout(attn, p=dropout_p, training=training)
        return attn

    attention = mask_softmax_dropout(attn_output_weights)  # [2b * num_head, l, src_l]
    if maam is not None:
        attn_output_weights -= maam(attention)  # [2b * num_head, l, src_l]
        attention = mask_softmax_dropout(attn_output_weights)  # [2b * num_head, l, src_l]

    # [2b * num_head, l, src_l] * [2b * num_head, src_l, head_dim]
    attn_output = torch.bmm(attention, v)  # [2b * num_head, l, head_dim]
    assert list(attn_output.size()) == [bsz * num_heads, tgt_len, head_dim]
    attn_output = attn_output.transpose(0, 1).contiguous().view(
        tgt_len, bsz, embed_dim)  # [l, 2b, d] d = num_head * head_dim
    attn_output = F.linear(attn_output, out_proj_weight, out_proj_bias)  # [l, 2b, d]

    if need_weights:
        return attn_output, attention  # [l, 2b, d], [2b * num_head, l, src_l]
    else:
        return attn_output, None
